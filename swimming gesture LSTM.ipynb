{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90cfd8e7-dab5-452d-b50d-5c13684cdf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cae834cf-88c0-4760-9849-f9b11a64c26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®å·²ä¿å­˜ï¼\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# âœ… æ–‡ä»¶è·¯å¾„\n",
    "file_paths = {\n",
    "    \"Freestyle\": \"/Users/zhuoshangming/Desktop/freestyle--swim_data.csv\",\n",
    "    \"Breaststroke\": \"/Users/zhuoshangming/Desktop/breakstroke--swim_data.csv\",\n",
    "    \"Backstroke\": \"/Users/zhuoshangming/Desktop/Backstroke--swim_data.csv\",\n",
    "}\n",
    "\n",
    "# âœ… è¯»å– & å½’ä¸€åŒ–æ•°æ®\n",
    "dataframes = []\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for swim_type, path in file_paths.items():\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"Stroke\"] = swim_type\n",
    "    df = df.sort_values(by=[df.columns[0]])  # æŒ‰æ—¶é—´æˆ³å‡åºæ’åº\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    dataframes.append(df)\n",
    "\n",
    "df_all = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# å½’ä¸€åŒ– X, Y, Z\n",
    "df_all[[\"X\", \"Y\", \"Z\"]] = scaler.fit_transform(df_all[[\"X\", \"Y\", \"Z\"]])\n",
    "\n",
    "# âœ… ä¿å­˜å½’ä¸€åŒ–æ¨¡å‹\n",
    "scaler_path = \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/scaler.pkl\"\n",
    "os.makedirs(os.path.dirname(scaler_path), exist_ok=True)\n",
    "import joblib\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "# âœ… æŒ‰ 15 ä¸ªæ•°ä¸€ç»„\n",
    "GROUP_SIZE = 15\n",
    "groups = [df_all.iloc[i:i+GROUP_SIZE] for i in range(0, len(df_all), GROUP_SIZE)]\n",
    "valid_groups = [g for g in groups if len(g[\"Stroke\"].unique()) == 1]\n",
    "\n",
    "# âœ… åˆ’åˆ†è®­ç»ƒé›† & æµ‹è¯•é›†ï¼ˆ**ä¸æ‰“ä¹±é¡ºåº**ï¼‰\n",
    "test_data = valid_groups[:24]  # å‰ 24 ç»„\n",
    "train_data = valid_groups[24:]  # å‰©ä½™éƒ¨åˆ†\n",
    "\n",
    "# âœ… ä¿å­˜æ•°æ®\n",
    "output_dir = \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "pd.concat(train_data, ignore_index=True).to_csv(os.path.join(output_dir, \"train_data.csv\"), index=False)\n",
    "pd.concat(test_data, ignore_index=True).to_csv(os.path.join(output_dir, \"test_data.csv\"), index=False)\n",
    "\n",
    "print(\"âœ… è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®å·²ä¿å­˜ï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9d3ef0c-df1e-4d6b-805f-aa2b2aaaed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.7353 - loss: 0.8798 - val_accuracy: 1.0000 - val_loss: 0.3784\n",
      "Epoch 2/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.5025 - val_accuracy: 1.0000 - val_loss: 0.1821\n",
      "Epoch 3/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.2446 - val_accuracy: 1.0000 - val_loss: 0.0825\n",
      "Epoch 4/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.1137 - val_accuracy: 1.0000 - val_loss: 0.0456\n",
      "Epoch 5/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0564 - val_accuracy: 1.0000 - val_loss: 0.0313\n",
      "Epoch 6/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0368 - val_accuracy: 1.0000 - val_loss: 0.0237\n",
      "Epoch 7/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0253 - val_accuracy: 1.0000 - val_loss: 0.0184\n",
      "Epoch 8/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0222 - val_accuracy: 1.0000 - val_loss: 0.0145\n",
      "Epoch 9/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0136 - val_accuracy: 1.0000 - val_loss: 0.0123\n",
      "Epoch 10/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0123 - val_accuracy: 1.0000 - val_loss: 0.0103\n",
      "Epoch 11/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0134 - val_accuracy: 1.0000 - val_loss: 0.0089\n",
      "Epoch 12/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0099 - val_accuracy: 1.0000 - val_loss: 0.0075\n",
      "Epoch 13/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0080 - val_accuracy: 1.0000 - val_loss: 0.0065\n",
      "Epoch 14/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0085 - val_accuracy: 1.0000 - val_loss: 0.0055\n",
      "Epoch 15/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0067 - val_accuracy: 1.0000 - val_loss: 0.0048\n",
      "Epoch 16/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 0.0043\n",
      "Epoch 17/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 0.0039\n",
      "Epoch 18/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0036\n",
      "Epoch 19/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0033\n",
      "Epoch 20/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0030\n",
      "Epoch 21/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0028\n",
      "Epoch 22/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0025\n",
      "Epoch 23/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0023\n",
      "Epoch 24/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0021\n",
      "Epoch 25/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0020\n",
      "Epoch 26/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0018\n",
      "Epoch 27/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0017\n",
      "Epoch 28/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
      "Epoch 29/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
      "Epoch 30/30\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LSTM è®­ç»ƒå®Œæˆå¹¶ä¿å­˜ï¼\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# âœ… è¯»å–æ•°æ®\n",
    "train_data_path = \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/train_data.csv\"\n",
    "df_train = pd.read_csv(train_data_path)\n",
    "\n",
    "# âœ… åŠ è½½å½’ä¸€åŒ–æ¨¡å‹\n",
    "scaler_path = \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/scaler.pkl\"\n",
    "scaler = joblib.load(scaler_path)\n",
    "df_train[[\"X\", \"Y\", \"Z\"]] = scaler.transform(df_train[[\"X\", \"Y\", \"Z\"]])\n",
    "\n",
    "# âœ… ç‰¹å¾æå–\n",
    "def extract_sequences(df):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(df), 15):\n",
    "        subset = df.iloc[i:i + 15]\n",
    "        if len(subset) == 15 and len(subset[\"Stroke\"].unique()) == 1:\n",
    "            X.append(subset[[\"X\", \"Y\", \"Z\"]].values)\n",
    "            y.append(subset[\"Stroke\"].values[0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = extract_sequences(df_train)\n",
    "\n",
    "# âœ… æ ‡ç­¾ç¼–ç \n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# âœ… LSTM æ¨¡å‹\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(15, 3)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# âœ… è®­ç»ƒæ¨¡å‹\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=16, validation_split=0.2)\n",
    "\n",
    "# âœ… ä¿å­˜æ¨¡å‹\n",
    "model_path = \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/swim_lstm_model.h5\"\n",
    "model.save(model_path)\n",
    "joblib.dump(label_encoder, \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/label_encoder.pkl\")\n",
    "\n",
    "print(\"âœ… LSTM è®­ç»ƒå®Œæˆå¹¶ä¿å­˜ï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86913bbb-0ecd-4dca-80fd-382d937cad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "ğŸ¯ LSTM æµ‹è¯•å‡†ç¡®ç‡: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# âœ… è¯»å–æµ‹è¯•æ•°æ®\n",
    "test_data_path = \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/test_data.csv\"\n",
    "df_test = pd.read_csv(test_data_path)\n",
    "\n",
    "# âœ… åŠ è½½æ¨¡å‹\n",
    "model_path = \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/swim_lstm_model.h5\"\n",
    "scaler_path = \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/scaler.pkl\"\n",
    "label_encoder_path = \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/label_encoder.pkl\"\n",
    "\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "label_encoder = joblib.load(label_encoder_path)\n",
    "\n",
    "df_test[[\"X\", \"Y\", \"Z\"]] = scaler.transform(df_test[[\"X\", \"Y\", \"Z\"]])\n",
    "\n",
    "# âœ… æå–æµ‹è¯•é›†ç‰¹å¾\n",
    "def extract_sequences(df):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(df), 15):\n",
    "        subset = df.iloc[i:i + 15]\n",
    "        if len(subset) == 15 and len(subset[\"Stroke\"].unique()) == 1:\n",
    "            X.append(subset[[\"X\", \"Y\", \"Z\"]].values)\n",
    "            y.append(subset[\"Stroke\"].values[0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_test, y_test = extract_sequences(df_test)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# âœ… è¿›è¡Œé¢„æµ‹\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = label_encoder.inverse_transform(np.argmax(y_pred, axis=1))\n",
    "\n",
    "# âœ… è®¡ç®—å‡†ç¡®ç‡\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test_encoded, np.argmax(y_pred, axis=1))\n",
    "print(f\"ğŸ¯ LSTM æµ‹è¯•å‡†ç¡®ç‡: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0f40953-79f4-4bc7-9f11-9767c867f88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "âœ… æœªçŸ¥æ•°æ®é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ /Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/Test_Predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# âœ… è¯»å–æœªçŸ¥æ•°æ®\n",
    "test_unknown_path = \"/Users/zhuoshangming/Desktop/Test.csv\"\n",
    "df_unknown = pd.read_csv(test_unknown_path)\n",
    "\n",
    "# âœ… åŠ è½½æ¨¡å‹å’Œæ ‡å‡†åŒ–\n",
    "model_path = \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/swim_lstm_model.h5\"\n",
    "scaler_path = \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/scaler.pkl\"\n",
    "label_encoder_path = \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/label_encoder.pkl\"\n",
    "\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "label_encoder = joblib.load(label_encoder_path)\n",
    "\n",
    "# âœ… å½’ä¸€åŒ– `X, Y, Z`\n",
    "df_unknown[[\"X\", \"Y\", \"Z\"]] = scaler.transform(df_unknown[[\"X\", \"Y\", \"Z\"]])\n",
    "\n",
    "# âœ… æå– `15` ä¸ªæ•°çš„ç‰¹å¾\n",
    "def extract_sequences(df):\n",
    "    X = []\n",
    "    indices = []\n",
    "    for i in range(0, len(df) - 14, 15):  # ç¡®ä¿æ¯ç»„15ä¸ª\n",
    "        subset = df.iloc[i:i + 15]\n",
    "        if len(subset) == 15:\n",
    "            X.append(subset[[\"X\", \"Y\", \"Z\"]].values)\n",
    "            indices.append((i, i + 15))  # è®°å½•åŸå§‹ç´¢å¼•\n",
    "    return np.array(X), indices\n",
    "\n",
    "X_unknown, indices = extract_sequences(df_unknown)\n",
    "\n",
    "# âœ… è¿›è¡Œé¢„æµ‹\n",
    "y_unknown_pred = model.predict(X_unknown)\n",
    "y_unknown_labels = label_encoder.inverse_transform(np.argmax(y_unknown_pred, axis=1))\n",
    "\n",
    "# âœ… å¤„ç†é¢„æµ‹ç»“æœï¼Œå¡«å…… `df_unknown`\n",
    "predicted_stroke_column = [\"\"] * len(df_unknown)\n",
    "\n",
    "for (start_idx, end_idx), label in zip(indices, y_unknown_labels):\n",
    "    predicted_stroke_column[start_idx:end_idx] = [label] * 15  # å¡«å…… 15 ä¸ªæ•°\n",
    "\n",
    "df_unknown[\"Predicted Stroke\"] = predicted_stroke_column\n",
    "\n",
    "# âœ… ä¿å­˜é¢„æµ‹ç»“æœ\n",
    "output_unknown_path = \"/Users/zhuoshangming/Desktop/è®­ç»ƒæ•°æ®åŸå§‹å¤„ç†/Test_Predictions.csv\"\n",
    "df_unknown.to_csv(output_unknown_path, index=False)\n",
    "\n",
    "print(f\"âœ… æœªçŸ¥æ•°æ®é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ {output_unknown_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c06e8-0b9e-48b2-a4d0-31137b5ca704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
